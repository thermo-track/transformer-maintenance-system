version: '3.8'

services:
  inference-api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: tms-inference-api
    ports:
      - "8001:8001"
    volumes:
      # Mount weights directory to persist model files
      - ./weights:/app/weights:ro
      # Mount outputs directory to access results
      - ./outputs:/app/outputs
    environment:
      - PYTHONUNBUFFERED=1
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import requests; requests.get('http://localhost:8001/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - tms-network

networks:
  tms-network:
    external: true
